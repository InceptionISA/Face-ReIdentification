{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89993,"databundleVersionId":10699058,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data Consistency Checking","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nBASE_DIR = '/kaggle/input/surveillance-for-retail-stores/face_identification/face_identification'\nTRAIN_DIR = os.path.join(BASE_DIR, 'train')\nTEST_DIR = os.path.join(BASE_DIR, 'test')\n\ntrain_csv_path = os.path.join(BASE_DIR, 'trainset.csv')\neval_csv_path = os.path.join(BASE_DIR, 'eval_set.csv')\n\ntrain_df = pd.read_csv(train_csv_path)\neval_df = pd.read_csv(eval_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T21:05:11.138793Z","iopub.execute_input":"2025-03-02T21:05:11.139105Z","iopub.status.idle":"2025-03-02T21:05:12.103235Z","shell.execute_reply.started":"2025-03-02T21:05:11.139073Z","shell.execute_reply":"2025-03-02T21:05:12.102484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=== Checking Training Set Consistency ===\")\n# For training images, CSV paths are like \"train/person_100/85.jpg\"\nmissing_train_files = []\nfor idx, row in train_df.iterrows():\n    csv_path = row['image_path']\n    full_path = os.path.join(BASE_DIR, csv_path)\n    if not os.path.exists(full_path):\n        missing_train_files.append(full_path)\n\nif missing_train_files:\n    print(\"Missing training files (from CSV):\")\n    for f in missing_train_files:\n        print(f\"  - {f}\")\nelse:\n    print(\"All files listed in trainset.csv exist.\")\n\n# Check for extra files in TRAIN_DIR not in CSV\ncsv_train_files = set()\nfor path in train_df['image_path']:\n    rel_path = path[len(\"train/\"):] if path.startswith(\"train/\") else path\n    csv_train_files.add(rel_path)\n\nactual_train_files = []\nfor root, dirs, files in os.walk(TRAIN_DIR):\n    for file in files:\n        rel_path = os.path.relpath(os.path.join(root, file), TRAIN_DIR)\n        actual_train_files.append(rel_path)\n\nextra_train_files = set(actual_train_files) - csv_train_files\n\nif extra_train_files:\n    print(\"\\nExtra training files in the train directory (not listed in CSV):\")\n    for f in extra_train_files:\n        print(f\"  - {f}\")\nelse:\n    print(\"No extra training files found; CSV matches the train directory.\")\n\n# === Checking Evaluation Set Consistency ===\nprint(\"\\n=== Checking Evaluation Set Consistency ===\")\n# For evaluation, CSV image names (e.g., \"9198.jpg\") should be in TEST_DIR\nmissing_eval_files = []\nfor idx, row in eval_df.iterrows():\n    image_name = row['image_path']\n    full_path = os.path.join(TEST_DIR, image_name)\n    if not os.path.exists(full_path):\n        missing_eval_files.append(full_path)\n\nif missing_eval_files:\n    print(\"Missing evaluation files (from CSV):\")\n    for f in missing_eval_files:\n        print(f\"  - {f}\")\nelse:\n    print(\"All files listed in eval_set.csv exist in the test directory.\")\n\n# Check for extra files in TEST_DIR not in CSV\ncsv_eval_files = set(eval_df['image_path'].tolist())\nactual_eval_files = [f for f in os.listdir(TEST_DIR) if os.path.isfile(os.path.join(TEST_DIR, f))]\nextra_eval_files = set(actual_eval_files) - csv_eval_files\n\nif extra_eval_files:\n    print(\"\\nExtra evaluation files in the test directory (not listed in CSV):\")\n    for f in extra_eval_files:\n        print(f\"  - {f}\")\nelse:\n    print(\"No extra evaluation files found; CSV matches the test directory.\")\n\n# === Numerical Report ===\nprint(\"\\n=== Numerical Report ===\")\n\n# Training Set Statistics\nprint(\"\\n--- Training Set Statistics ---\")\ntotal_train_images = len(train_df)\nprint(f\"Total images in CSV: {total_train_images}\")\n\n# Extract person_id (e.g., \"train/person_100/85.jpg\" -> \"person_100\")\ntrain_df['person_id'] = train_df['image_path'].apply(lambda x: x.split('/')[1])\nunique_persons_train = train_df['person_id'].nunique()\nprint(f\"Unique persons: {unique_persons_train}\")\n\n# Distribution of images per person\nimages_per_person = train_df.groupby('person_id').size()\nprint(\"Images per person:\")\nprint(f\"  Min: {images_per_person.min()}\")\nprint(f\"  Max: {images_per_person.max()}\")\nprint(f\"  Mean: {images_per_person.mean():.2f}\")\nprint(f\"  Median: {images_per_person.median()}\")\nprint(f\"  Std Dev: {images_per_person.std():.2f}\")\n\nprint(f\"Missing files: {len(missing_train_files)}\")\nprint(f\"Extra files: {len(extra_train_files)}\")\n\n# Test Set Statistics\nprint(\"\\n--- Test Set Statistics ---\")\ntotal_test_images = len(eval_df)\nprint(f\"Total images in CSV: {total_test_images}\")\n\nif 'label' in eval_df.columns:\n    unique_persons_test = eval_df['label'].nunique()\n    print(f\"Unique persons: {unique_persons_test}\")\n    train_persons = set(train_df['person_id'])\n    test_persons = set(eval_df['label'])\n    unseen_persons = test_persons - train_persons\n    print(f\"Unseen persons: {len(unseen_persons)}\")\nelse:\n    print(\"Unique persons: N/A (no 'label' column)\")\n\nprint(f\"Missing files: {len(missing_eval_files)}\")\nprint(f\"Extra files: {len(extra_eval_files)}\")\n\n# Summary\nprint(\"\\n--- Summary ---\")\nprint(f\"Training images: {total_train_images} (unique persons: {unique_persons_train})\")\nprint(f\"Test images: {total_test_images}\")\nif 'label' in eval_df.columns:\n    print(f\"Test unique persons: {unique_persons_test} (unseen: {len(unseen_persons)})\")\nelse:\n    print(\"Test unique persons: N/A\")\nprint(f\"Total missing files: {len(missing_train_files) + len(missing_eval_files)}\")\nprint(f\"Total extra files: {len(extra_train_files) + len(extra_eval_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T21:05:12.104230Z","iopub.execute_input":"2025-03-02T21:05:12.104443Z","iopub.status.idle":"2025-03-02T21:05:41.189447Z","shell.execute_reply.started":"2025-03-02T21:05:12.104424Z","shell.execute_reply":"2025-03-02T21:05:41.188677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<br><br><br><br>","metadata":{}},{"cell_type":"markdown","source":"### Data Is Consistant, We can move forward now","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install deepface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T21:05:41.190707Z","iopub.execute_input":"2025-03-02T21:05:41.190927Z","iopub.status.idle":"2025-03-02T21:05:49.044941Z","shell.execute_reply.started":"2025-03-02T21:05:41.190908Z","shell.execute_reply":"2025-03-02T21:05:49.044005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Creating Validation Set","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport statistics\n\n# Organize dataset into training and validation sets\ntrain_files, val_files = {}, {}\n\nprint(\"Organizing dataset...\")\n\nperson_dirs = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\nfor person in person_dirs:\n    person_path = os.path.join(TRAIN_DIR, person)\n    images = [os.path.join(person_path, f) for f in os.listdir(person_path) if os.path.isfile(os.path.join(person_path, f))]\n    if not images:\n        continue\n    random.shuffle(images)\n    split_idx = int(0.8 * len(images))\n    train_files[person], val_files[person] = images[:split_idx], images[split_idx:]\n\n# Function to generate report\ndef generate_report(file_dict, dataset_name):\n    total_images = sum(len(images) for images in file_dict.values())\n    num_persons = len(file_dict)\n    person_image_counts = [len(images) for images in file_dict.values() if len(images) > 0]\n    \n    if person_image_counts:\n        min_images = min(person_image_counts)\n        max_images = max(person_image_counts)\n        mean_images = total_images / num_persons\n        median_images = statistics.median(person_image_counts)\n    else:\n        min_images = 0\n        max_images = 0\n        mean_images = 0\n        median_images = 0\n    \n    print(f\"\\n{dataset_name} Dataset Report:\")\n    print(f\"Total number of images: {total_images}\")\n    print(f\"Number of unique persons: {num_persons}\")\n    print(\"Images per person:\")\n    print(f\"  Minimum: {min_images}\")\n    print(f\"  Maximum: {max_images}\")\n    print(f\"  Mean: {mean_images:.2f}\")\n    print(f\"  Median: {median_images}\")\n\n# Generate reports for training and validation sets\ngenerate_report(train_files, \"Training\")\ngenerate_report(val_files, \"Validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T21:05:49.046408Z","iopub.execute_input":"2025-03-02T21:05:49.046693Z","iopub.status.idle":"2025-03-02T21:05:52.506287Z","shell.execute_reply.started":"2025-03-02T21:05:49.046657Z","shell.execute_reply":"2025-03-02T21:05:52.505552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<br><br><br><br>","metadata":{}},{"cell_type":"markdown","source":"### DeepFace Embedding Similarity Trial","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\nfrom tqdm import tqdm\nfrom deepface import DeepFace\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n\n# Configuration\nMODELS = ['Facenet', 'Facenet512', 'VGG-Face', 'Dlib']\nEMBEDDINGS_DIR = 'embeddings'\nBACKEND = 'retinaface'\nUSE_ALIGNMENT = True\nSIMILARITY_THRESHOLD = 0.3\nRANDOM_SEED = 42\n\n# Ensure embeddings directory exists\nos.makedirs(EMBEDDINGS_DIR, exist_ok=True)\nfor model_name in MODELS:\n    os.makedirs(os.path.join(EMBEDDINGS_DIR, model_name), exist_ok=True)\n\nrandom.seed(RANDOM_SEED)\n\ndef get_embedding_deepface(image_path, model_name='Facenet512'):\n    \"\"\"Compute embedding using DeepFace with specified model and RetinaFace backend\"\"\"\n    try:\n        reps = DeepFace.represent(\n            img_path=image_path, \n            model_name=model_name,\n            detector_backend=BACKEND,\n            align=USE_ALIGNMENT,\n            enforce_detection=False\n        )\n        if reps and 'embedding' in reps[0]:\n            return np.array(reps[0]['embedding'])\n    except Exception as e:\n        print(f\"Error processing {image_path} with {model_name}: {e}\")\n    return None\n\ndef compute_and_save_embeddings(file_dict, dataset_type, model_name):\n    \"\"\"Compute embeddings for all images and save to files\"\"\"\n    embeddings_path = os.path.join(EMBEDDINGS_DIR, model_name, f\"{dataset_type}_embeddings.npz\")\n    \n    # Check if embeddings already exist\n    if os.path.exists(embeddings_path):\n        print(f\"Loading existing {model_name} embeddings for {dataset_type} dataset\")\n        return np.load(embeddings_path, allow_pickle=True)\n    \n    print(f\"Computing {model_name} embeddings for {dataset_type} dataset...\")\n    embeddings = {}\n    person_embeddings = {}\n    image_paths = {}\n    \n    for person in tqdm(file_dict, desc=f\"Processing {dataset_type} embeddings with {model_name}\"):\n        person_embeddings[person] = []\n        image_paths[person] = []\n        \n        for img_path in file_dict[person]:\n            emb = get_embedding_deepface(img_path, model_name=model_name)\n            if emb is not None:\n                person_embeddings[person].append(emb)\n                image_paths[person].append(img_path)\n        \n        if person_embeddings[person]:\n            # Store the mean embedding for each person\n            embeddings[person] = np.mean(person_embeddings[person], axis=0)\n    \n    # Save embeddings to file\n    np.savez(\n        embeddings_path,\n        embeddings=embeddings,\n        person_embeddings=person_embeddings,\n        image_paths=image_paths\n    )\n    \n    print(f\"Saved {model_name} embeddings for {dataset_type} dataset to {embeddings_path}\")\n    return np.load(embeddings_path, allow_pickle=True)\n\ndef predict_person(test_emb, train_embeddings, threshold=SIMILARITY_THRESHOLD):\n    \"\"\"Predict identity using cosine similarity\"\"\"\n    similarities = {\n        person: cosine_similarity(test_emb.reshape(1, -1), emb.reshape(1, -1))[0][0] \n        for person, emb in train_embeddings.items()\n    }\n    \n    if not similarities:\n        return \"unknown\"\n    \n    pred_person, max_sim = max(similarities.items(), key=lambda x: x[1])\n    return pred_person if max_sim >= threshold else \"unknown\"\n\ndef evaluate_model(train_files, val_files, model_name):\n    \"\"\"Evaluate model performance on training and validation sets\"\"\"\n    # Load or compute embeddings\n    train_data = compute_and_save_embeddings(train_files, 'train', model_name)\n    val_data = compute_and_save_embeddings(val_files, 'val', model_name)\n    \n    train_embeddings = train_data['embeddings'].item()\n    \n    results = {}\n    \n    # Evaluate on training set\n    print(f\"\\nEvaluating {model_name} on training set...\")\n    train_true, train_pred = [], []\n    \n    for person in tqdm(train_files, desc=\"Processing training evaluation\"):\n        for img_path in train_files[person]:\n            emb = get_embedding_deepface(img_path, model_name=model_name)\n            if emb is None:\n                continue\n            pred = predict_person(emb, train_embeddings)\n            train_true.append(person)\n            train_pred.append(pred)\n    \n    # Evaluate on validation set\n    print(f\"\\nEvaluating {model_name} on validation set...\")\n    val_true, val_pred = [], []\n    \n    for person in tqdm(val_files, desc=\"Processing validation evaluation\"):\n        for img_path in val_files[person]:\n            emb = get_embedding_deepface(img_path, model_name=model_name)\n            if emb is None:\n                continue\n            pred = predict_person(emb, train_embeddings)\n            val_true.append(person)\n            val_pred.append(pred)\n    \n    # Calculate metrics\n    train_acc = accuracy_score(train_true, train_pred)\n    train_f1 = f1_score(train_true, train_pred, average='weighted', zero_division=0)\n    train_recall = recall_score(train_true, train_pred, average='weighted', zero_division=0)\n    train_precision = precision_score(train_true, train_pred, average='weighted', zero_division=0)\n    \n    val_acc = accuracy_score(val_true, val_pred)\n    val_f1 = f1_score(val_true, val_pred, average='weighted', zero_division=0)\n    val_recall = recall_score(val_true, val_pred, average='weighted', zero_division=0)\n    val_precision = precision_score(val_true, val_pred, average='weighted', zero_division=0)\n    \n    # Print metrics\n    print(f\"\\n{model_name} Results:\")\n    print(f\"Training set - Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}, Recall: {train_recall:.4f}, Precision: {train_precision:.4f}\")\n    print(f\"Validation set - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}\")\n    \n    # Print detailed classification report for validation set\n    print(f\"\\nDetailed {model_name} Classification Report for Validation Set:\")\n    print(classification_report(val_true, val_pred, zero_division=0))\n    \n    # Store results\n    results = {\n        'model_name': model_name,\n        'train_true': train_true,\n        'train_pred': train_pred,\n        'val_true': val_true,\n        'val_pred': val_pred,\n        'train_metrics': {\n            'accuracy': train_acc,\n            'f1': train_f1,\n            'recall': train_recall,\n            'precision': train_precision\n        },\n        'val_metrics': {\n            'accuracy': val_acc,\n            'f1': val_f1,\n            'recall': val_recall,\n            'precision': val_precision\n        }\n    }\n    \n    # Save results to file\n    results_path = os.path.join(EMBEDDINGS_DIR, model_name, 'results.npz')\n    np.savez(results_path, **results)\n    print(f\"Saved {model_name} results to {results_path}\")\n    \n    return results\n\ndef run_full_pipeline(train_files, val_files):\n    \"\"\"Run the full pipeline for all models\"\"\"\n    all_results = {}\n    \n    for model_name in MODELS:\n        print(f\"\\n{'='*50}\")\n        print(f\"Processing model: {model_name}\")\n        print(f\"{'='*50}\")\n        \n        start_time = time.time()\n        results = evaluate_model(train_files, val_files, model_name)\n        end_time = time.time()\n        \n        elapsed_time = end_time - start_time\n        print(f\"Time taken for {model_name}: {elapsed_time:.2f} seconds\")\n        \n        all_results[model_name] = results\n    \n    # Save summary of all results\n    summary = {\n        model_name: {\n            'train_metrics': results['train_metrics'],\n            'val_metrics': results['val_metrics']\n        }\n        for model_name, results in all_results.items()\n    }\n    \n    summary_df = pd.DataFrame([\n        {\n            'Model': model_name,\n            'Train Accuracy': results['train_metrics']['accuracy'],\n            'Train F1': results['train_metrics']['f1'],\n            'Train Recall': results['train_metrics']['recall'],\n            'Train Precision': results['train_metrics']['precision'],\n            'Val Accuracy': results['val_metrics']['accuracy'],\n            'Val F1': results['val_metrics']['f1'],\n            'Val Recall': results['val_metrics']['recall'],\n            'Val Precision': results['val_metrics']['precision']\n        }\n        for model_name, results in all_results.items()\n    ])\n    \n    # Save summary to CSV\n    summary_path = os.path.join(EMBEDDINGS_DIR, 'model_comparison_summary.csv')\n    summary_df.to_csv(summary_path, index=False)\n    print(f\"\\nSaved model comparison summary to {summary_path}\")\n    \n    # Print summary table\n    print(\"\\nModel Comparison Summary:\")\n    print(summary_df.to_string(index=False))\n    \n    return all_results\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"Initializing face recognition pipeline...\")\n    print(\"Starting multi-model evaluation...\")\n    all_results = run_full_pipeline(train_files, val_files)\n    print(\"Process completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T21:05:52.507304Z","iopub.execute_input":"2025-03-02T21:05:52.507652Z","execution_failed":"2025-03-02T21:07:05.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results Visualizations","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom collections import Counter\nimport os\nimport argparse\n\ndef generate_person_classification_summary(train_true, train_pred, val_true, val_pred):\n    \"\"\"Generate summary statistics for each person in the dataset\"\"\"\n    train_freq = Counter(train_true)\n    val_freq = Counter(val_true)\n    \n    persons = np.unique([p for p in np.unique(np.concatenate([train_true, val_true])) \n                         if 'person' in str(p)])\n    results = []\n    \n    for person in persons:\n        y_true_train = (np.array(train_true) == person).astype(int)\n        y_pred_train = (np.array(train_pred) == person).astype(int)\n        \n        train_precision = precision_score(y_true_train, y_pred_train, zero_division=0)\n        train_recall = recall_score(y_true_train, y_pred_train, zero_division=0)\n        train_f1 = f1_score(y_true_train, y_pred_train, zero_division=0)\n        \n        y_true_val = (np.array(val_true) == person).astype(int)\n        y_pred_val = (np.array(val_pred) == person).astype(int)\n        \n        val_precision = precision_score(y_true_val, y_pred_val, zero_division=0)\n        val_recall = recall_score(y_true_val, y_pred_val, zero_division=0)\n        val_f1 = f1_score(y_true_val, y_pred_val, zero_division=0)\n        \n        person_id = int(str(person).split('_')[-1])\n        \n        results.append({\n            'person': person,\n            'person_id': person_id,\n            'train_frequency': train_freq.get(person, 0),\n            'train_precision': train_precision,\n            'train_recall': train_recall,\n            'train_f1': train_f1,\n            'val_frequency': val_freq.get(person, 0),\n            'val_precision': val_precision,\n            'val_recall': val_recall,\n            'val_f1': val_f1\n        })\n    \n    results_df = pd.DataFrame(results).sort_values('person_id')\n    \n    return results_df\n\ndef plot_metrics_by_person(train_true, train_pred, val_true, val_pred, model_name, save_dir):\n    \"\"\"Plot F1, precision, and recall metrics for each person\"\"\"\n    train_true = np.array(train_true)\n    train_pred = np.array(train_pred)\n    val_true = np.array(val_true)\n    val_pred = np.array(val_pred)\n    \n    summary_df = generate_person_classification_summary(train_true, train_pred, val_true, val_pred)\n    \n    person_ids = summary_df['person_id'].values\n    train_f1_values = summary_df['train_f1'].values\n    val_f1_values = summary_df['val_f1'].values\n    train_precision_values = summary_df['train_precision'].values\n    val_precision_values = summary_df['val_precision'].values\n    train_recall_values = summary_df['train_recall'].values\n    val_recall_values = summary_df['val_recall'].values\n    train_freqs = summary_df['train_frequency'].values\n    val_freqs = summary_df['val_frequency'].values\n\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 24))\n    \n    # Plot 1: F1 Scores - Blue and Red (as is)\n    ax1.plot(person_ids, train_f1_values, 'b-', label='Training F1 Score', marker='o', alpha=0.7)\n    ax1.plot(person_ids, val_f1_values, 'r-', label='Validation F1 Score', marker='o', alpha=0.7)\n    \n    train_min_indices = np.argsort(train_f1_values)[:2]\n    val_min_indices = np.argsort(val_f1_values)[:3]\n    \n    for i, idx in enumerate(train_min_indices):\n        ax1.annotate(f'F1: {train_f1_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {train_freqs[idx]}',\n                     xy=(person_ids[idx], train_f1_values[idx]),\n                     xytext=(person_ids[idx], train_f1_values[idx] - 0.15),\n                     arrowprops=dict(facecolor='blue', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='blue', ha='center', \n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"blue\", alpha=0.8))\n    \n    for i, idx in enumerate(val_min_indices):\n        y_offset = -0.15 - (i * 0.05)\n        ax1.annotate(f'F1: {val_f1_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {val_freqs[idx]}',\n                     xy=(person_ids[idx], val_f1_values[idx]),\n                     xytext=(person_ids[idx], val_f1_values[idx] + y_offset),\n                     arrowprops=dict(facecolor='red', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='red', ha='center', \n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", alpha=0.8))\n    \n    ax1.set_xlabel('Person ID')\n    ax1.set_ylabel('F1 Score')\n    ax1.set_title(f'F1 Scores by Person (Training vs Validation) - {model_name}')\n    ax1.set_xticks(person_ids[::5])\n    ax1.grid(True, alpha=0.3)\n    ax1.legend()\n    ax1.set_ylim(min(min(train_f1_values), min(val_f1_values)) - 0.3, \n                 max(max(train_f1_values), max(val_f1_values)) + 0.25)\n    \n    train_avg_f1 = np.mean(train_f1_values)\n    val_avg_f1 = np.mean(val_f1_values)\n    textstr_f1 = (f'Training Avg F1: {train_avg_f1:.3f}\\n'\n                  f'Validation Avg F1: {val_avg_f1:.3f}\\n'\n                  f'Annotated points:\\n'\n                  f'  Training: 2 lowest\\n'\n                  f'  Validation: 3 lowest')\n    ax1.text(0.05, 0.95, textstr_f1, transform=ax1.transAxes, fontsize=10,\n             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    # Plot 2: Precision - Orange and Blue\n    ax2.plot(person_ids, train_precision_values, 'C1-', marker='o', label='Training Precision', alpha=0.7, color='orange')\n    ax2.plot(person_ids, val_precision_values, 'b-', marker='o', label='Validation Precision', alpha=0.7)\n    \n    train_min_indices = np.argsort(train_precision_values)[:2]\n    val_min_indices = np.argsort(val_precision_values)[:3]\n    \n    for i, idx in enumerate(train_min_indices):\n        ax2.annotate(f'Prec: {train_precision_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {train_freqs[idx]}',\n                     xy=(person_ids[idx], train_precision_values[idx]),\n                     xytext=(person_ids[idx], train_precision_values[idx] - 0.15),\n                     arrowprops=dict(facecolor='orange', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='darkorange', ha='center',\n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"orange\", alpha=0.8))\n    \n    for i, idx in enumerate(val_min_indices):\n        y_offset = -0.15 - (i * 0.05)\n        ax2.annotate(f'Prec: {val_precision_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {val_freqs[idx]}',\n                     xy=(person_ids[idx], val_precision_values[idx]),\n                     xytext=(person_ids[idx], val_precision_values[idx] + y_offset),\n                     arrowprops=dict(facecolor='blue', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='blue', ha='center',\n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"blue\", alpha=0.8))\n    \n    ax2.set_xlabel('Person ID')\n    ax2.set_ylabel('Precision')\n    ax2.set_title(f'Precision by Person (Training vs Validation) - {model_name}')\n    ax2.set_xticks(person_ids[::5])\n    ax2.grid(True, alpha=0.3)\n    ax2.legend()\n    ax2.set_ylim(min(min(train_precision_values), min(val_precision_values)) - 0.3, \n                 max(max(train_precision_values), max(val_precision_values)) + 0.25)\n    \n    train_avg_prec = np.mean(train_precision_values)\n    val_avg_prec = np.mean(val_precision_values)\n    textstr_prec = (f'Training Avg Precision: {train_avg_prec:.3f}\\n'\n                    f'Validation Avg Precision: {val_avg_prec:.3f}\\n'\n                    f'Annotated points:\\n'\n                    f'  Training: 2 lowest\\n'\n                    f'  Validation: 3 lowest')\n    ax2.text(0.05, 0.95, textstr_prec, transform=ax2.transAxes, fontsize=10,\n             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    # Plot 3: Recall - Black and Green\n    ax3.plot(person_ids, train_recall_values, 'k-', marker='o', label='Training Recall', alpha=0.7)\n    ax3.plot(person_ids, val_recall_values, 'g-', marker='o', label='Validation Recall', alpha=0.7)\n    \n    train_min_indices = np.argsort(train_recall_values)[:2]\n    val_min_indices = np.argsort(val_recall_values)[:3]\n    \n    for i, idx in enumerate(train_min_indices):\n        ax3.annotate(f'Recall: {train_recall_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {train_freqs[idx]}',\n                     xy=(person_ids[idx], train_recall_values[idx]),\n                     xytext=(person_ids[idx], train_recall_values[idx] - 0.15),\n                     arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='black', ha='center',\n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n    \n    for i, idx in enumerate(val_min_indices):\n        y_offset = -0.15 - (i * 0.05)\n        ax3.annotate(f'Recall: {val_recall_values[idx]:.2f}\\nPerson {person_ids[idx]}\\nFreq: {val_freqs[idx]}',\n                     xy=(person_ids[idx], val_recall_values[idx]),\n                     xytext=(person_ids[idx], val_recall_values[idx] + y_offset),\n                     arrowprops=dict(facecolor='green', shrink=0.05, width=1.5, headwidth=8),\n                     fontsize=9, color='green', ha='center',\n                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"green\", alpha=0.8))\n    \n    ax3.set_xlabel('Person ID')\n    ax3.set_ylabel('Recall')\n    ax3.set_title(f'Recall by Person (Training vs Validation) - {model_name}')\n    ax3.set_xticks(person_ids[::5])\n    ax3.grid(True, alpha=0.3)\n    ax3.legend()\n    ax3.set_ylim(min(min(train_recall_values), min(val_recall_values)) - 0.3, \n                 max(max(train_recall_values), max(val_recall_values)) + 0.25)\n    \n    train_avg_recall = np.mean(train_recall_values)\n    val_avg_recall = np.mean(val_recall_values)\n    textstr_recall = (f'Training Avg Recall: {train_avg_recall:.3f}\\n'\n                      f'Validation Avg Recall: {val_avg_recall:.3f}\\n'\n                      f'Annotated points:\\n'\n                      f'  Training: 2 lowest\\n'\n                      f'  Validation: 3 lowest')\n    ax3.text(0.05, 0.95, textstr_recall, transform=ax3.transAxes, fontsize=10,\n             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    \n    # Create directory for saving plots\n    os.makedirs(save_dir, exist_ok=True)\n    save_path = os.path.join(save_dir, f'{model_name}_person_metrics.png')\n    \n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"Plots for {model_name} saved to {save_path}\")\n    \n    # Save summary data to CSV\n    summary_path = os.path.join(save_dir, f'{model_name}_person_metrics.csv')\n    summary_df.to_csv(summary_path, index=False)\n    print(f\"Summary data for {model_name} saved to {summary_path}\")\n    \n    # Print lowest performers\n    train_lowest = summary_df.nsmallest(2, 'train_f1')\n    val_lowest = summary_df.nsmallest(3, 'val_f1')\n\n    print(f\"\\n2 lowest performers in {model_name} training set:\")\n    print(train_lowest[['person_id', 'train_frequency', 'train_f1']])\n\n    print(f\"\\n3 lowest performers in {model_name} validation set:\")\n    print(val_lowest[['person_id', 'val_frequency', 'val_f1']])\n    \n    return fig, summary_df\n\ndef plot_model_comparison(models_results, save_dir):\n    \"\"\"Plot comparison of models performance\"\"\"\n    # Extract metrics for all models\n    models = list(models_results.keys())\n    train_f1 = [models_results[model]['train_metrics']['f1'] for model in models]\n    val_f1 = [models_results[model]['val_metrics']['f1'] for model in models]\n    train_precision = [models_results[model]['train_metrics']['precision'] for model in models]\n    val_precision = [models_results[model]['val_metrics']['precision'] for model in models]\n    train_recall = [models_results[model]['train_metrics']['recall'] for model in models]\n    val_recall = [models_results[model]['val_metrics']['recall'] for model in models]\n    \n    # Set up bar plot\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))\n    \n    # F1 Score comparison\n    x = np.arange(len(models))\n    width = 0.35\n    \n    ax1.bar(x - width/2, train_f1, width, label='Training F1', color='blue', alpha=0.7)\n    ax1.bar(x + width/2, val_f1, width, label='Validation F1', color='red', alpha=0.7)\n    ax1.set_xlabel('Model')\n    ax1.set_ylabel('F1 Score')\n    ax1.set_title('F1 Score Comparison Across Models')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(models)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for i, v in enumerate(train_f1):\n        ax1.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    for i, v in enumerate(val_f1):\n        ax1.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Precision comparison\n    ax2.bar(x - width/2, train_precision, width, label='Training Precision', color='orange', alpha=0.7)\n    ax2.bar(x + width/2, val_precision, width, label='Validation Precision', color='blue', alpha=0.7)\n    ax2.set_xlabel('Model')\n    ax2.set_ylabel('Precision')\n    ax2.set_title('Precision Comparison Across Models')\n    ax2.set_xticks(x)\n    ax2.set_xticklabels(models)\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for i, v in enumerate(train_precision):\n        ax2.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    for i, v in enumerate(val_precision):\n        ax2.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Recall comparison\n    ax3.bar(x - width/2, train_recall, width, label='Training Recall', color='black', alpha=0.7)\n    ax3.bar(x + width/2, val_recall, width, label='Validation Recall', color='green', alpha=0.7)\n    ax3.set_xlabel('Model')\n    ax3.set_ylabel('Recall')\n    ax3.set_title('Recall Comparison Across Models')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(models)\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for i, v in enumerate(train_recall):\n        ax3.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    for i, v in enumerate(val_recall):\n        ax3.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    plt.tight_layout()\n    \n    # Save comparison plot\n    os.makedirs(save_dir, exist_ok=True)\n    save_path = os.path.join(save_dir, 'model_comparison.png')\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"Model comparison plot saved to {save_path}\")\n    \n    # Create a summary dataframe\n    summary_df = pd.DataFrame({\n        'Model': models,\n        'Training F1': train_f1,\n        'Validation F1': val_f1,\n        'Training Precision': train_precision,\n        'Validation Precision': val_precision,\n        'Training Recall': train_recall,\n        'Validation Recall': val_recall\n    })\n    \n    # Save summary to CSV\n    summary_path = os.path.join(save_dir, 'model_comparison_metrics.csv')\n    summary_df.to_csv(summary_path, index=False)\n    print(f\"Model comparison data saved to {summary_path}\")\n    \n    return fig, summary_df\n\ndef analyze_results_from_file(embeddings_dir, models, plots_dir='plots'):\n    \"\"\"Load results from files and generate visualizations\"\"\"\n    all_results = {}\n    \n    for model_name in models:\n        results_path = os.path.join(embeddings_dir, model_name, 'results.npz')\n        \n        if not os.path.exists(results_path):\n            print(f\"Results file for {model_name} not found at {results_path}\")\n            continue\n        \n        try:\n            data = np.load(results_path, allow_pickle=True)\n            results = {key: data[key] for key in data.files}\n            \n            # Plot individual model results\n            plot_metrics_by_person(\n                results['train_true'], \n                results['train_pred'], \n                results['val_true'], \n                results['val_pred'],\n                model_name,\n                plots_dir\n            )\n            \n            all_results[model_name] = {\n                'train_metrics': results['train_metrics'].item(),\n                'val_metrics': results['val_metrics'].item()\n            }\n            \n        except Exception as e:\n            print(f\"Error processing results for {model_name}: {e}\")\n    \n    # Generate model comparison plot\n    if all_results:\n        plot_model_comparison(all_results, plots_dir)\n    \n    return all_results\n\nif __name__ == \"__main__\":\n    embeddings_dir = 'embeddings'  # Directory containing model embeddings and results\n    plots_dir = 'plots'            # Directory to save plots\n    models = ['Facenet', 'Facenet512', 'VGG-Face', 'Dlib']  # List of models to analyze\n    \n    print(f\"Analyzing results for models: {models}\")\n    results = analyze_results_from_file(embeddings_dir, models, plots_dir)\n    \n    if not results:\n        print(\"No results were found. Please ensure the model evaluation has been run first.\")\n    else:\n        print(\"Analysis completed successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-02T21:07:05.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}